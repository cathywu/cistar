{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 0: Flow\n",
    "\n",
    "This tutorial serves as an introduction to Flow. It will give you a better understanding about what Flow is and how it works. Whether you want to be serious about using Flow or wish to contribute to the project, it is important that you first understand the basics about how our code is organized and what it does. This tutorial will introduce you to the core concepts used in Flow and is a highly recommended read before you dive into the following tutorials.\n",
    "\n",
    "**How to get help:** If you happen, throughout the tutorials or when building your own code, to have any general or technical question related to Flow, don't hesitate to have a look on [Stack Overflow](https://stackoverflow.com/questions/tagged/flow-project) and see if it has been answered already, or otherwise to post it using the tag `flow-project`. We will be happy to help you!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. High-level flow\n",
    "\n",
    "<img src=\"img/flow_venn_diagram.png\" alt=\"Flow Venn Diagram\" width=\"50%\"/>\n",
    "\n",
    "Flow acts as a bridge between a traffic simulator (e.g. Sumo, Aimsun...) and a reinforcement learning library (e.g. RLlib, rllab...). It provides you with an interface that lets you train agents on a custom road network without having to worry about integration with the traffic simulator and the learning library. Flow creates this connection automatically, and also provides you with some tools to analyze the trained policies.\n",
    "\n",
    "In order to get started and train your own agent on your own road network, all you will need is: \n",
    "\n",
    "- <u>**a scenario**</u>: this is basically the term we use to talk about a road network. A scenario is a class that contains information about the road network on which your agents will be trained. It describes the roads (position, size, number of lanes, speed limit...), the connections between the roads (junctions, intersections...) and possibly other information (traffic lights...).\n",
    "\n",
    "- <u>**an environment**</u>: this is an RL environment _(**not to be confused** with the physical environment that we call scenario)_. It is a class that allows you to control how the agent will be trained. You should define and can then tune: a **state space** (what the agent can see, for instance the position and speed of all vehicles in the network, or the difference of speed with the vehicle in front of the agent), an **action space** (what the agent can do, for instance a list of accelerations to apply to all RL vehicles, or a list of new states for all traffic lights), and a **reward function** (what the agent will try to maximize, for instance the mean speed of all vehicles in the network).\n",
    "\n",
    "_Flow already contains a dozen of pre-defined scenarios and their corresponding environments that you can re-use or use as models to build your own scenarios and environments._\n",
    "\n",
    "Once you have defined these two classes, the last step is to set up the parameters of the simulation, for instance:\n",
    "\n",
    "- number of iterations (for the RL algorithm), number of rollouts (simulations) in each iteration, horizon (number of steps in each rollout)\n",
    "\n",
    "- vehicles to add in the network (human vehicles, RL vehicles, vehicle inflows...), color sequences for traffic lights (if there are any)\n",
    "\n",
    "- Flow parameters: name of the simulation, scenario and environment classes to be used, simulator to use (Sumo, Aimsun...)\n",
    "\n",
    "- RL algorithm to use (PPO, TRPO...), parameters of the algorithm (discount rate, neural network parameters...), number of CPUs/GPUs to use, checkpoint frequency, whether or not to restart from an old checkpoint (to resume training or do transfer learning)\n",
    "\n",
    "- whether or not to render the simulation (if you don't see anything happening in the simulator when you run the experiment, it is because rendering has been disabled, which makes training significantly faster)\n",
    "\n",
    "It is also possible to run a simulation without doing any training, in this case you won't need any environment or RL algorithm.\n",
    "\n",
    "Flow has a lot of examples setting up all these simulation parameters, so you can simply take one of them and modify it according to your needs. Once this is done, you can execute the Python file where the experiment is set up and the training shall begin. \n",
    "\n",
    "During the training or after it has ended, you can use Flow's visualization tools in order to visualize the data saved in the checkpoint files generated during the training. You can see how well your agent is doing by running a new simulation in the simulator, that will used the trained policy (this time, the simulation will be rendered). You can also plot the reward or return functions, time-space diagrams, capacity diagrams etc.\n",
    "\n",
    "In the next section, we will give an overview of how Flow's codebase is organized, so that you can have some reference marks when you go through the following tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Codebase structure\n",
    "\n",
    "The `flow` directory is structured as follows:\n",
    "\n",
    "```python\n",
    "flow\n",
    "├── docs  # some random documents, don't worry about it\n",
    "├── examples  # a lot of example codes using Flow -- this is where you want to head once you're done with the tutorials and want to start doing some real code\n",
    "│   ├── aimsun  # examples using just Aimsun, without training \n",
    "│   ├── rllab  # examples of training with rllab (avoid rllab, we are going to deprecate it soon!)\n",
    "│   ├── rllib  # examples of training with RLlib\n",
    "│   └── sumo  # examples using just SUMO, without training\n",
    "├── flow\n",
    "│   ├── benchmarks  # several custom networks and configurations on which you can evaluate and compare different RL algorithms\n",
    "│   ├── controllers  # implementations of controllers for the vehicles (IDM, Follower-Stopper...)\n",
    "│   ├── core  # where the magic happens\n",
    "│   │   └── kernel\n",
    "│   │       ├── scenario  # logic for the scenario\n",
    "│   │       ├── simulation  # where the simulation is created and managed \n",
    "│   │       ├── traffic_light  # logic for the traffic lights\n",
    "│   │       └── vehicle  # logic for the vehicles\n",
    "│   ├── envs  # environments (where states, actions and rewards are handled)\n",
    "│   ├── multiagent_envs  # multi-agent environments\n",
    "│   ├── renderer  # pyglet renderer\n",
    "│   ├── scenarios  # scenarios (ie road networks)\n",
    "│   ├── utils  # the files that don't fit anywhere else\n",
    "│   └── visualize  # scripts to replay policies, analyse reward functions etc.\n",
    "├── scripts  # mostly installation scripts\n",
    "├── tests  # unit tests\n",
    "└── tutorials  # <-- you are here\n",
    "```\n",
    "\n",
    "Don't hesitate to go and read the code files directly, we try to keep everything commented and understandable. However if something remains unclear, even after reading all the tutorials and going through the examples, you can ask us on [Stack Overflow](https://stackoverflow.com/questions/tagged/flow-project) using the tag `flow-project` (make sure your question wasn't already asked before!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
