#!/bin/bash

#SBATCH -o test.out-%j
#SBATCH --job-name=test
#SBATCH -n 16

export LC_ALL=C.UTF-8
export LANG=C.UTF-8

((worker_num=$SLURM_NNODES-1)) # Must be one less the total number of nodes
echo $worker_num

source /etc/profile
module load anaconda3-2019a
unset PYTHONPATH

#export LD_library_path if needed

eval "$(conda shell.bash hook)"
conda activate flow

nodes=$(scontrol show hostnames $SLURM_JOB_NODELIST) # Getting the node names
nodes_array=( $nodes )

node1=${nodes_array[0]}

# Getting the IP address of the first node that is being used
ip_prefix=$(srun --nodes=1 --ntasks=1 -w $node1 hostname --ip-address) # Making redis-address
suffix=':6379'
echo $suffix

# IP address + the port number
ip_head=$ip_prefix$suffix

# Exporting for latter access by the python script you actually want to run
export ip_head 

# Putting temp files generated by Ray to a designated place
headtemp=$(srun --nodes=1 --ntasks=1 -w $node1 echo $TMPDIR) 
srun --nodes=1 --ntasks=1 -w $node1 ray start --temp-dir=$headtemp --block --head --redis-port=6379 & # Starting the head
sleep 5

for ((  i=1; i<=$worker_num; i++ ))
do
    node2=${nodes_array[$i]}
    workertemp='workertemp-'$node2
    #workertemp=$(srun --nodes=1 --ntasks=1 -w $node2 echo $TMPDIR) # used in original Lauren's script
    srun --nodes=1 --ntasks=1 -w $node2 ray start --temp-dir=$workertemp --block --redis-address=$ip_head & # Starting the workers
done
which python
# If this is running on an interactive node, you can either replace $SLURM_NTASKS with the acutal
# number of CPUs you want to use or use the environment variable $SLURM_CPUS_ON_NODE
# but also you need to make sure if the specified number here matches #CPUs specified in your python 
# script
python examples/rllib/stabilizing_the_ring.py $SLURM_NTASKS # Pass the total number of allocated CPUs
